{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import gensim\n",
    "from gensim import utils\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "nlp = spacy.load(\"en\")\n",
    "nlp.max_length = 3100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished loading negative data...\n"
     ]
    }
   ],
   "source": [
    "negList = []\n",
    "for file in os.listdir(\"./neg\"):\n",
    "    if \".txt\" in file:\n",
    "        f = open(\"./neg/\"+file)\n",
    "        negList.append(f.read())\n",
    "print(\"finished loading negative data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished loading positive data...\n"
     ]
    }
   ],
   "source": [
    "posList = []\n",
    "for file in os.listdir(\"./pos\"):\n",
    "    if \".txt\" in file:\n",
    "        f = open(\"./pos/\"+file)\n",
    "        posList.append(f.read())\n",
    "print(\"finished loading positive data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "negTokenList = []\n",
    "wordDict = {}\n",
    "for review in negList:\n",
    "    doc = nlp(review)\n",
    "    reviewTokenized = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ != \"PUNCT\" and token.pos_ != \"SPACE\" and token.pos_ != \"SYM\":\n",
    "            word = token.text.lower()\n",
    "            reviewTokenized += \" \" + word\n",
    "            count = wordDict.get(word,0)\n",
    "            wordDict[word] = count+1\n",
    "    negTokenList.append(reviewTokenized[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "posTokenList = []\n",
    "for review in posList:\n",
    "    doc = nlp(review)\n",
    "    reviewTokenized = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ != \"PUNCT\" and token.pos_ != \"SPACE\" and token.pos_ != \"SYM\":\n",
    "            word = token.text.lower()\n",
    "            reviewTokenized += \" \" + word\n",
    "            count = wordDict.get(word,0)\n",
    "            wordDict[word] = count+1\n",
    "    posTokenList.append(reviewTokenized[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"negTokenList.json\",\"w\") as fp:\n",
    "    json.dump(negTokenList,fp)\n",
    "with open(\"posTokenList.json\",\"w\") as fp:\n",
    "    json.dump(posTokenList,fp)\n",
    "with open(\"wordCount.json\",\"w\") as fp:\n",
    "    json.dump(wordDict,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "negTokenList = []\n",
    "posTokenList = []\n",
    "wordDict = {}\n",
    "with open(\"negTokenList.json\",\"r\") as fp:\n",
    "    negTokenList = json.load(fp)\n",
    "with open(\"posTokenList.json\",\"r\") as fp:\n",
    "    posTokenList = json.load(fp)\n",
    "with open(\"wordCount.json\",\"r\") as fp:\n",
    "    wordDict = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "allReviews = negTokenList + posTokenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedWords = sorted(wordDict.items(),key = lambda x : x[1],reverse=True)\n",
    "vocabIndex={w:i+1 for i,(w,c) in enumerate(sortedWords)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedReviews=list()\n",
    "for r in allReviews:\n",
    "    encodedReview=list()\n",
    "    for word in r.split(\" \"):\n",
    "         #if word is not available in vocab_to_int put 0 in that place else use the index\n",
    "        encodedReview.append(vocabIndex.get(word,0))\n",
    "    encodedReviews.append(encodedReview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNcustom(nn.Module):\n",
    "    def __init__(self, input_dim, layer_dims, output_dim, activation):\n",
    "        super(NNcustom, self).__init__()\n",
    "        self.num_layer = len(layer_dims)\n",
    "        self.activation = activation\n",
    "        if activation is not \"LINEAR\":\n",
    "            self.act = self.getActivation(activation)\n",
    "\n",
    "        self.inLayer = nn.Linear(input_dim, layer_dims[0])\n",
    "        self.linears = nn.ModuleList([self.act])\n",
    "        for i in range(self.num_layer - 1):\n",
    "            self.linears.extend([nn.Linear(layer_dims[i],layer_dims[i+1])])\n",
    "        self.outLayer = nn.Linear(layer_dims[self.num_layer - 1], output_dim)\n",
    "        self.lastactivation = nn.Sigmoid()\n",
    "\n",
    "    def getActivation(self, activation):\n",
    "        if(activation is \"RELU\"):\n",
    "            return nn.ReLU()\n",
    "        if(activation is \"SIG\"):\n",
    "            return nn.Sigmoid()\n",
    "        if (activation is \"TANH\"):\n",
    "            return nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.inLayer(x)\n",
    "        for l in self.linears:\n",
    "            out = l(out)\n",
    "            if self.activation is not \"LINEAR\":\n",
    "                out = self.act(out)\n",
    "        out = self.outLayer(out)\n",
    "        out = self.lastactivation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_x_nn,train_y_nn,epoch=3,itr=1000):\n",
    "    learning_rate = 0.02\n",
    "    error = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    for e in range(epoch):\n",
    "        for t in range(itr):\n",
    "            out = model(train_x_nn)\n",
    "            loss = error(out, train_y_nn)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "        print('Epoch [%d/%d] ========== loss: %.3f' %(e + 1, epoch,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,test_x_nn,test_y_nn):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_x_nn)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += test_y_nn.size(0)\n",
    "        correct += (predicted == test_y_nn).sum().item()\n",
    "        return(100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tenFoldPartition(i,x_data,y_data):\n",
    "    if i < 0:\n",
    "        return x_data, y_data,x_data,y_data\n",
    "    test_to = int(1000 * i)\n",
    "    test_from = int(1000 * (i + 1))\n",
    "\n",
    "    train_x_1=x_data[:test_to]\n",
    "    train_x_2=x_data[test_from:]\n",
    "    train_x = np.concatenate((train_x_1,train_x_2))\n",
    "    \n",
    "    train_y_1=y_data[:test_to]\n",
    "    train_y_2=y_data[test_from:]\n",
    "    train_y=np.concatenate((train_y_1,train_y_2))\n",
    "\n",
    "    test_x=x_data[test_to:test_from]\n",
    "    test_y=y_data[test_to:test_from]\n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 fold cross validation\n",
    "\n",
    "def tenFoldTrain(x_data,y_data):\n",
    "    input_dim = 300\n",
    "    output_dim = 2\n",
    "    layerList = [[200,100,10],[150,75,10],[100,50,10]]\n",
    "    actList = [\"RELU\",\"TANH\",\"SIG\"]\n",
    "    for layer_dims in layerList:\n",
    "        for act in actList:\n",
    "            for i in range(0,10):\n",
    "                train_x,train_y,test_x,test_y = tenFoldPartition(i,x_data,y_data)\n",
    "                print(\"Validation data \" + str(i*1000) + \" \" + str((i+1)*1000))\n",
    "                train_x_nn = torch.from_numpy(train_x).float()\n",
    "                train_y_nn = Variable(torch.as_tensor(train_y,dtype=torch.long).long())\n",
    "                test_x_nn = torch.from_numpy(test_x).float()\n",
    "                test_y_nn = Variable(torch.as_tensor(test_y,dtype=torch.long).long())\n",
    "                model = NNcustom(input_dim, layer_dims, output_dim,act)\n",
    "                learning_rate = 0.02\n",
    "                error = nn.CrossEntropyLoss()\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "                print(\"Training with layers size \",layer_dims, \" and activation function \", act);\n",
    "                train(model,train_x_nn,train_y_nn,6,5000)\n",
    "                accuracy = predict(model,test_x_nn,test_y_nn)\n",
    "                print(\"Accuracy: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTrainingFullData(x_data,y_data):\n",
    "    input_dim = 300\n",
    "    output_dim = 2\n",
    "    layer_dims = [150,75,10]\n",
    "    act = \"TANH\"\n",
    "    print(\"Training on complete data with \", act,\" activation function and \",layer_dims, \"as size of intermediate layers\")\n",
    "    train_x_nn = torch.from_numpy(x_data).float()\n",
    "    train_y_nn = Variable(torch.as_tensor(y_data,dtype=torch.long).long())\n",
    "    test_x_nn = torch.from_numpy(x_data).float()\n",
    "    test_y_nn = Variable(torch.as_tensor(y_data,dtype=torch.long).long())\n",
    "    model = NNcustom(input_dim, layer_dims, output_dim,act)\n",
    "    train(model,train_x_nn,train_y_nn,10,10000)\n",
    "    accuracy = predict(model,test_x_nn,test_y_nn)\n",
    "    print(\"Accuracy: \" + str(accuracy))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram Features\n",
    "Q3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length=300\n",
    "features=np.zeros((len(encodedReviews), sequence_length), dtype=int)\n",
    "for i, review in enumerate(encodedReviews):\n",
    "    review_len=len(review)\n",
    "    if (review_len<=sequence_length):\n",
    "        zeros=list(np.zeros(sequence_length-review_len))\n",
    "        new=review+zeros\n",
    "    else:\n",
    "        new=review[:sequence_length]\n",
    "    features[i,:]=np.array(new)\n",
    "z = [0]*5000\n",
    "o = [1]*5000\n",
    "labels = np.array(z+o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data 0 1000\n",
      "Training with layers size  [200, 50, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.669\n",
      "Epoch [2/3] ========== loss: 0.639\n",
      "Epoch [3/3] ========== loss: 0.604\n",
      "Accuracy: 49.4\n",
      "Validation data 1000 2000\n",
      "Training with layers size  [200, 50, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.665\n",
      "Epoch [2/3] ========== loss: 0.639\n",
      "Epoch [3/3] ========== loss: 0.602\n",
      "Accuracy: 48.9\n",
      "Validation data 2000 3000\n",
      "Training with layers size  [200, 50, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.672\n",
      "Epoch [2/3] ========== loss: 0.644\n",
      "Epoch [3/3] ========== loss: 0.616\n",
      "Accuracy: 46.6\n",
      "Validation data 3000 4000\n",
      "Training with layers size  [200, 50, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.664\n",
      "Epoch [2/3] ========== loss: 0.637\n",
      "Epoch [3/3] ========== loss: 0.602\n",
      "Accuracy: 49.0\n",
      "Validation data 4000 5000\n",
      "Training with layers size  [200, 50, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.672\n",
      "Epoch [2/3] ========== loss: 0.644\n",
      "Epoch [3/3] ========== loss: 0.615\n",
      "Accuracy: 46.2\n",
      "Validation data 5000 6000\n",
      "Training with layers size  [200, 50, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.668\n",
      "Epoch [2/3] ========== loss: 0.637\n",
      "Epoch [3/3] ========== loss: 0.597\n",
      "Accuracy: 45.3\n",
      "Validation data 6000 7000\n",
      "Training with layers size  [200, 50, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.671\n",
      "Epoch [2/3] ========== loss: 0.643\n",
      "Epoch [3/3] ========== loss: 0.614\n",
      "Accuracy: 46.6\n",
      "Validation data 7000 8000\n",
      "Training with layers size  [200, 50, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.669\n",
      "Epoch [2/3] ========== loss: 0.635\n",
      "Epoch [3/3] ========== loss: 0.593\n",
      "Accuracy: 46.8\n",
      "Validation data 8000 9000\n",
      "Training with layers size  [200, 50, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.671\n",
      "Epoch [2/3] ========== loss: 0.639\n",
      "Epoch [3/3] ========== loss: 0.598\n",
      "Accuracy: 46.7\n",
      "Validation data 9000 10000\n",
      "Training with layers size  [200, 50, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.672\n",
      "Epoch [2/3] ========== loss: 0.634\n",
      "Epoch [3/3] ========== loss: 0.586\n",
      "Accuracy: 46.4\n",
      "Validation data 0 1000\n",
      "Training with layers size  [150, 75, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.663\n",
      "Epoch [2/3] ========== loss: 0.634\n",
      "Epoch [3/3] ========== loss: 0.592\n",
      "Accuracy: 48.5\n",
      "Validation data 1000 2000\n",
      "Training with layers size  [150, 75, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.674\n",
      "Epoch [2/3] ========== loss: 0.649\n",
      "Epoch [3/3] ========== loss: 0.624\n",
      "Accuracy: 47.5\n",
      "Validation data 2000 3000\n",
      "Training with layers size  [150, 75, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.666\n",
      "Epoch [2/3] ========== loss: 0.634\n",
      "Epoch [3/3] ========== loss: 0.596\n",
      "Accuracy: 46.4\n",
      "Validation data 3000 4000\n",
      "Training with layers size  [150, 75, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.669\n",
      "Epoch [2/3] ========== loss: 0.643\n",
      "Epoch [3/3] ========== loss: 0.612\n",
      "Accuracy: 46.5\n",
      "Validation data 4000 5000\n",
      "Training with layers size  [150, 75, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.661\n",
      "Epoch [2/3] ========== loss: 0.636\n",
      "Epoch [3/3] ========== loss: 0.612\n",
      "Accuracy: 45.6\n",
      "Validation data 5000 6000\n",
      "Training with layers size  [150, 75, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.674\n",
      "Epoch [2/3] ========== loss: 0.644\n",
      "Epoch [3/3] ========== loss: 0.604\n",
      "Accuracy: 49.8\n",
      "Validation data 6000 7000\n",
      "Training with layers size  [150, 75, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.657\n",
      "Epoch [2/3] ========== loss: 0.624\n",
      "Epoch [3/3] ========== loss: 0.582\n",
      "Accuracy: 49.6\n",
      "Validation data 7000 8000\n",
      "Training with layers size  [150, 75, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.674\n",
      "Epoch [2/3] ========== loss: 0.640\n",
      "Epoch [3/3] ========== loss: 0.596\n",
      "Accuracy: 48.6\n",
      "Validation data 8000 9000\n",
      "Training with layers size  [150, 75, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.670\n",
      "Epoch [2/3] ========== loss: 0.640\n",
      "Epoch [3/3] ========== loss: 0.601\n",
      "Accuracy: 48.8\n",
      "Validation data 9000 10000\n",
      "Training with layers size  [150, 75, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.682\n",
      "Epoch [2/3] ========== loss: 0.664\n",
      "Epoch [3/3] ========== loss: 0.633\n",
      "Accuracy: 45.7\n",
      "Validation data 0 1000\n",
      "Training with layers size  [100, 50, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.674\n",
      "Epoch [2/3] ========== loss: 0.652\n",
      "Epoch [3/3] ========== loss: 0.626\n",
      "Accuracy: 43.4\n",
      "Validation data 1000 2000\n",
      "Training with layers size  [100, 50, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.678\n",
      "Epoch [2/3] ========== loss: 0.657\n",
      "Epoch [3/3] ========== loss: 0.630\n",
      "Accuracy: 41.8\n",
      "Validation data 2000 3000\n",
      "Training with layers size  [100, 50, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.672\n",
      "Epoch [2/3] ========== loss: 0.647\n",
      "Epoch [3/3] ========== loss: 0.623\n",
      "Accuracy: 46.4\n",
      "Validation data 3000 4000\n",
      "Training with layers size  [100, 50, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.678\n",
      "Epoch [2/3] ========== loss: 0.657\n",
      "Epoch [3/3] ========== loss: 0.627\n",
      "Accuracy: 44.8\n",
      "Validation data 4000 5000\n",
      "Training with layers size  [100, 50, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.672\n",
      "Epoch [2/3] ========== loss: 0.648\n",
      "Epoch [3/3] ========== loss: 0.619\n",
      "Accuracy: 42.3\n",
      "Validation data 5000 6000\n",
      "Training with layers size  [100, 50, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.678\n",
      "Epoch [2/3] ========== loss: 0.655\n",
      "Epoch [3/3] ========== loss: 0.619\n",
      "Accuracy: 45.6\n",
      "Validation data 6000 7000\n",
      "Training with layers size  [100, 50, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.678\n",
      "Epoch [2/3] ========== loss: 0.655\n",
      "Epoch [3/3] ========== loss: 0.619\n",
      "Accuracy: 47.7\n",
      "Validation data 7000 8000\n",
      "Training with layers size  [100, 50, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.681\n",
      "Epoch [2/3] ========== loss: 0.660\n",
      "Epoch [3/3] ========== loss: 0.617\n",
      "Accuracy: 47.5\n",
      "Validation data 8000 9000\n",
      "Training with layers size  [100, 50, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.678\n",
      "Epoch [2/3] ========== loss: 0.657\n",
      "Epoch [3/3] ========== loss: 0.629\n",
      "Accuracy: 47.9\n",
      "Validation data 9000 10000\n",
      "Training with layers size  [100, 50, 10]  and activation function  TANH\n",
      "Epoch [1/3] ========== loss: 0.672\n",
      "Epoch [2/3] ========== loss: 0.645\n",
      "Epoch [3/3] ========== loss: 0.619\n",
      "Accuracy: 44.7\n"
     ]
    }
   ],
   "source": [
    "tenFoldTrain(features,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on complete data with  TANH  activation function and  [150, 75, 10] as size of intermediate layers\n",
      "Epoch [1/10] ========== loss: 0.425\n",
      "Epoch [2/10] ========== loss: 0.432\n",
      "Epoch [3/10] ========== loss: 0.420\n",
      "Epoch [4/10] ========== loss: 0.401\n",
      "Epoch [5/10] ========== loss: 0.392\n",
      "Epoch [6/10] ========== loss: 0.388\n",
      "Epoch [7/10] ========== loss: 0.386\n",
      "Epoch [8/10] ========== loss: 0.384\n",
      "Epoch [9/10] ========== loss: 0.383\n",
      "Epoch [10/10] ========== loss: 0.382\n",
      "Accuracy: 93.24\n"
     ]
    }
   ],
   "source": [
    "unigramModel = runTrainingFullData(features,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Encodings\n",
    "Q3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2VecModel = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vEncodedReviews=list()\n",
    "for r in allReviews:\n",
    "    w2v=list()\n",
    "    for word in r.split(\" \"):\n",
    "        vec = np.zeros(300)\n",
    "        if word in word2VecModel:\n",
    "            vec = vec + word2VecModel[word]\n",
    "    w2vEncodedReviews.append(vec/(len(r.split(\" \"))))\n",
    "w2vfeatures=np.array(w2vEncodedReviews)\n",
    "\n",
    "z = [0]*5000\n",
    "o = [1]*5000\n",
    "labels = np.array(z+o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenFoldTrain(w2vfeatures,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on complete data with  TANH  activation function and  [150, 75, 10] as size of intermediate layers\n",
      "Epoch [1/10] ========== loss: 0.693\n",
      "Epoch [2/10] ========== loss: 0.693\n",
      "Epoch [3/10] ========== loss: 0.693\n",
      "Epoch [4/10] ========== loss: 0.693\n",
      "Epoch [5/10] ========== loss: 0.693\n",
      "Epoch [6/10] ========== loss: 0.693\n",
      "Epoch [7/10] ========== loss: 0.693\n",
      "Epoch [8/10] ========== loss: 0.693\n",
      "Epoch [9/10] ========== loss: 0.693\n",
      "Epoch [10/10] ========== loss: 0.693\n",
      "Accuracy: 60.4\n"
     ]
    }
   ],
   "source": [
    "word2VecModel = runTrainingFullData(w2vfeatures,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF SVD\n",
    "Q3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(allReviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tenFoldTrainSVD(x_data,y_data):\n",
    "    n_components = [300,200,50,10]\n",
    "    output_dim = 2\n",
    "    layer_dims = [150,75,10]\n",
    "    actList = [\"RELU\"]\n",
    "    for n in n_components:\n",
    "        svd = TruncatedSVD(n_components=n, n_iter=7, random_state=42)\n",
    "        svdOut = svd.fit_transform(x_data)\n",
    "        input_dim = n\n",
    "        for act in actList:\n",
    "            for i in range(0,10):\n",
    "                train_x,train_y,test_x,test_y = tenFoldPartition(i,svdOut,y_data)\n",
    "                print(\"Validation data \" + str(i*1000) + \" \" + str((i+1)*1000))\n",
    "                train_x_nn = torch.from_numpy(train_x).float()\n",
    "                train_y_nn = Variable(torch.as_tensor(train_y,dtype=torch.long))\n",
    "                test_x_nn = torch.from_numpy(test_x).float()\n",
    "                test_y_nn = Variable(torch.as_tensor(test_y,dtype=torch.long).long())\n",
    "                model = NNcustom(input_dim, layer_dims, output_dim,act)\n",
    "                print(\"Training with layers size \",layer_dims, \" and activation function \", n, \" components\");\n",
    "                train(model,train_x_nn,train_y_nn,5,5000)\n",
    "                accuracy = predict(model,test_x_nn,test_y_nn)\n",
    "                print(\"Accuracy: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTrainingFullDataSVD(x_data,y_data):\n",
    "    n = 300\n",
    "    input_dim = 300\n",
    "    output_dim = 2\n",
    "    layer_dims = [150,75,10]\n",
    "    svd = TruncatedSVD(n_components=n, n_iter=7, random_state=42)\n",
    "    x_data = svd.fit_transform(x_data)\n",
    "    act = \"RELU\"\n",
    "    print(\"Training on complete data with \",act,\" activation function and \",layer_dims, \"as size of intermediate layers and \",n,\"components\")\n",
    "    train_x_nn = torch.from_numpy(x_data).float()\n",
    "    train_y_nn = Variable(torch.as_tensor(y_data,dtype=torch.long).long())\n",
    "    test_x_nn = torch.from_numpy(x_data).float()\n",
    "    test_y_nn = Variable(torch.as_tensor(y_data,dtype=torch.long).long())\n",
    "    model = NNcustom(input_dim, layer_dims, output_dim,act)\n",
    "    train(model,train_x_nn,train_y_nn,7,10000)\n",
    "    accuracy = predict(model,test_x_nn,test_y_nn)\n",
    "    print(\"Accuracy: \" + str(accuracy))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenFoldTrainSVD(X,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on complete data with  RELU  activation function and  [150, 75, 10] as size of intermediate layers and  300 components\n",
      "Epoch [1/7] ========== loss: 0.691\n",
      "Epoch [2/7] ========== loss: 0.372\n",
      "Epoch [3/7] ========== loss: 0.358\n",
      "Epoch [4/7] ========== loss: 0.354\n",
      "Epoch [5/7] ========== loss: 0.351\n",
      "Epoch [6/7] ========== loss: 0.349\n",
      "Epoch [7/7] ========== loss: 0.347\n",
      "Accuracy: 97.07\n"
     ]
    }
   ],
   "source": [
    "svdModel = runTrainingFullDataSVD(X,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 topics with most important words\n",
      "Topic  1\n",
      "1 surtout\n",
      "2 beaucoup\n",
      "3 bruyant\n",
      "4 recibir\n",
      "5 dijeron\n",
      "6 nuevo\n",
      "7 diferente\n",
      "8 pronto\n",
      "9 unas\n",
      "10 ea\n",
      "11 arrepentir\n",
      "12 propio\n",
      "13 suceda\n",
      "14 algo\n",
      "15 tuvimos\n",
      "16 pidió\n",
      "17 empleadas\n",
      "18 ordenar\n",
      "19 riendo\n",
      "20 buenísimo\n",
      "Topic  2\n",
      "1 is\n",
      "2 you\n",
      "3 are\n",
      "4 place\n",
      "5 great\n",
      "6 love\n",
      "7 this\n",
      "8 always\n",
      "9 best\n",
      "10 your\n",
      "11 have\n",
      "12 their\n",
      "13 here\n",
      "14 they\n",
      "15 staff\n",
      "16 friendly\n",
      "17 ve\n",
      "18 amazing\n",
      "19 can\n",
      "20 if\n",
      "Topic  3\n",
      "1 to\n",
      "2 me\n",
      "3 they\n",
      "4 my\n",
      "5 you\n",
      "6 she\n",
      "7 he\n",
      "8 do\n",
      "9 that\n",
      "10 have\n",
      "11 car\n",
      "12 them\n",
      "13 told\n",
      "14 her\n",
      "15 get\n",
      "16 if\n",
      "17 your\n",
      "18 up\n",
      "19 call\n",
      "20 when\n",
      "Topic  4\n",
      "1 was\n",
      "2 the\n",
      "3 it\n",
      "4 not\n",
      "5 but\n",
      "6 chicken\n",
      "7 me\n",
      "8 that\n",
      "9 like\n",
      "10 of\n",
      "11 good\n",
      "12 my\n",
      "13 sauce\n",
      "14 tasted\n",
      "15 ordered\n",
      "16 buffet\n",
      "17 just\n",
      "18 meat\n",
      "19 rice\n",
      "20 sandwich\n",
      "Topic  5\n",
      "1 we\n",
      "2 you\n",
      "3 the\n",
      "4 they\n",
      "5 not\n",
      "6 of\n",
      "7 if\n",
      "8 us\n",
      "9 our\n",
      "10 do\n",
      "11 but\n",
      "12 your\n",
      "13 it\n",
      "14 were\n",
      "15 order\n",
      "16 like\n",
      "17 that\n",
      "18 are\n",
      "19 there\n",
      "20 or\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 5 topics with most important words\")\n",
    "for j in range(0,5):\n",
    "    print(\"Topic \",j+1)\n",
    "    for i,index in enumerate(svd.components_[j].argsort()[:20]):\n",
    "        print(i+1,vectorizer.get_feature_names()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished loading test data...\n"
     ]
    }
   ],
   "source": [
    "fileList = []\n",
    "testDataList = []\n",
    "for file in os.listdir(\"./test\"):\n",
    "    if \".txt\" in file:\n",
    "        f = open(\"./test/\"+file)\n",
    "        fileList.append(file)\n",
    "        testDataList.append(f.read())\n",
    "print(\"finished loading test data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "testReviews = []\n",
    "for review in testDataList: \n",
    "    doc = nlp(review)\n",
    "    reviewTokenized = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ != \"PUNCT\" and token.pos_ != \"SPACE\" and token.pos_ != \"SYM\":\n",
    "            word = token.text.lower()\n",
    "            reviewTokenized += \" \" + word\n",
    "    testReviews.append(reviewTokenized[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "testTFIDF = vectorizer.fit_transform(allReviews)\n",
    "svd = TruncatedSVD(n_components=300, n_iter=7, random_state=42)\n",
    "x_data = svd.fit_transform(testTFIDF)\n",
    "test_x_nn = torch.from_numpy(x_data).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = svdModel(test_x_nn)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    y_pred = predicted.tolist()\n",
    "result = zip(fileList,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = \"\"\n",
    "pos = \"\"\n",
    "for tup in result:\n",
    "    if tup[1] == 0:\n",
    "        neg = neg + \"\\n\" + tup[0] \n",
    "    else :\n",
    "        pos = pos + \"\\n\" + tup[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pos.txt\",\"w\") as fp:\n",
    "    fp.write(pos)\n",
    "with open(\"neg.txt\",\"w\") as fn:\n",
    "    fn.write(neg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
