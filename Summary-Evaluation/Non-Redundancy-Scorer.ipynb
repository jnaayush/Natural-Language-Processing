{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import sklearn\n",
    "import numpy as np\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 15000000\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import readability\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import textacy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import gensim\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./summary_quality/train_data.json\",'r') as fin:\n",
    "    train_content = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./summary_quality/test_data.json\",'r') as fin:\n",
    "    test_content = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeature1(wordList):\n",
    "    unigram = {}\n",
    "    for word in wordList:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            c = unigram.get(word,0)\n",
    "            unigram[word] = c+1\n",
    "    return max(unigram.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeature2(wordList):\n",
    "    bigrams = {}\n",
    "    for i in range(0,len(wordList) - 1):\n",
    "        bigram = (wordList[i],wordList[i+1])\n",
    "        c = bigrams.get(bigram,0)\n",
    "        bigrams[bigram] = c + 1\n",
    "    return max(bigrams.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2VecModel = gensim.models.KeyedVectors.load_word2vec_format('../Q3/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeature3(doc):\n",
    "    cosineList = []\n",
    "    sentences = []\n",
    "    for sent in doc.sents:\n",
    "        sentences.append(sent.text.lower())\n",
    "    if(len(sentences) == 1):\n",
    "        return 0\n",
    "    for i in range(0,len(sentences)-1):\n",
    "        numS1 = 0\n",
    "        vw1 = np.zeros(300)\n",
    "        numS2 = 0\n",
    "        vw2 = np.zeros(300)\n",
    "        for word in sentences[i].split(\" \"):\n",
    "            if word in word2VecModel:\n",
    "                numS1 += 1\n",
    "                vw1 = vw1 + word2VecModel[word]\n",
    "        vw1 = vw1/numS1\n",
    "        for word in sentences[i+1].split(\" \"):\n",
    "            if word in word2VecModel:\n",
    "                numS2 += 1\n",
    "                vw2 = vw2 + word2VecModel[word]\n",
    "        vw2 = vw2/numS2\n",
    "        cos_sim = dot(vw1, vw2)/(norm(vw1)*norm(vw2))\n",
    "        cosineList.append(cos_sim)\n",
    "    return max([x for x in cosineList if ~np.isnan(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeature4(wordList):\n",
    "    trigrams = {}\n",
    "    for i in range(0,len(wordList) - 2):\n",
    "        trigram = (wordList[i],wordList[i+1],wordList[i+2])\n",
    "        c = trigrams.get(trigram,0)\n",
    "        trigrams[trigram] = c + 1\n",
    "    return max(trigrams.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeature5(doc):\n",
    "    scores = readability.getmeasures(doc.text, lang='en')\n",
    "    return scores['sentence info']['wordtypes'] / scores['sentence info']['words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Cell for 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2VecModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-50413cd9ea80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"PUNCT\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"SPACE\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"SYM\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mwordList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgetFeature1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgetFeature2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgetFeature3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgetFeature4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgetFeature5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdocFeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-15284695a2c1>\u001b[0m in \u001b[0;36mgetFeature3\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mvw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword2VecModel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0mnumS1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mvw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvw1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mword2VecModel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word2VecModel' is not defined"
     ]
    }
   ],
   "source": [
    "docFeature = OrderedDict()\n",
    "for file in os.listdir(\"./summary_quality/summaries/\"):\n",
    "    f = open(\"./summary_quality/summaries/\"+file, encoding = \"ISO-8859-1\")\n",
    "    text = f.read()\n",
    "    doc = nlp(text)\n",
    "    wordList=[]\n",
    "    for token in doc:\n",
    "        if token.pos_ != \"PUNCT\" and token.pos_ != \"SPACE\" and token.pos_ != \"SYM\":\n",
    "            wordList.append(token.text.lower())\n",
    "    t = (getFeature1(wordList),getFeature2(wordList),getFeature3(doc),getFeature4(wordList),getFeature5(doc))\n",
    "    docFeature[file] = t\n",
    "    f.close()\n",
    "\n",
    "    #create train and test data\n",
    "feature_X_Y = OrderedDict()\n",
    "for k,v in sorted(train_content.items(), key=lambda x: x):\n",
    "    feature_X_Y[k] = (docFeature[k],v['nonredundancy'])\n",
    "test_X_Y = OrderedDict()\n",
    "for k,v in sorted(test_content.items(), key=lambda x: x):\n",
    "    test_X_Y[k] = (docFeature[k],v['nonredundancy'])\n",
    "test_x = []\n",
    "test_y = []\n",
    "for k,v in test_X_Y.items():\n",
    "    test_x.append([float(v[0][0]),float(v[0][1]),float(v[0][2]),float(v[0][3]),float(v[0][4])])\n",
    "    test_y.append(float(v[1]))\n",
    "    train_x = []\n",
    "train_y = []\n",
    "for k,v in feature_X_Y.items():\n",
    "    train_x.append([float(v[0][0]),float(v[0][1]),float(v[0][2]),float(v[0][3]),float(v[0][4])])\n",
    "    train_y.append(float(v[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run cell for 3 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docFeature = OrderedDict()\n",
    "for file in os.listdir(\"./summary_quality/summaries/\"):\n",
    "    f = open(\"./summary_quality/summaries/\"+file, encoding = \"ISO-8859-1\")\n",
    "    text = f.read()\n",
    "    doc = nlp(text)\n",
    "    wordList=[]\n",
    "    for token in doc:\n",
    "        if token.pos_ != \"PUNCT\" and token.pos_ != \"SPACE\" and token.pos_ != \"SYM\":\n",
    "            wordList.append(token.text.lower())\n",
    "    t = (getFeature1(wordList),getFeature2(wordList),getFeature3(doc))\n",
    "    docFeature[file] = t\n",
    "    f.close()\n",
    "\n",
    "    #create train and test data\n",
    "feature_X_Y = OrderedDict()\n",
    "for k,v in sorted(train_content.items(), key=lambda x: x):\n",
    "    feature_X_Y[k] = (docFeature[k],v['nonredundancy'])\n",
    "test_X_Y = OrderedDict()\n",
    "for k,v in sorted(test_content.items(), key=lambda x: x):\n",
    "    test_X_Y[k] = (docFeature[k],v['nonredundancy'])\n",
    "test_x = []\n",
    "test_y = []\n",
    "for k,v in test_X_Y.items():\n",
    "    test_x.append([float(v[0][0]),float(v[0][1]),float(v[0][2])])\n",
    "    test_y.append(float(v[1]))\n",
    "    train_x = []\n",
    "train_y = []\n",
    "for k,v in feature_X_Y.items():\n",
    "    train_x.append([float(v[0][0]),float(v[0][1]),float(v[0][2])])\n",
    "    train_y.append(float(v[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = SVR(kernel='rbf')\n",
    "regressor.fit(train_x,train_y)#5 Predicting a new result\n",
    "y_pred = regressor.predict(test_x)\n",
    "print(\"SVR MSE \",mean_squared_error(test_y,y_pred))\n",
    "print(\"SVR Pearson \",pearsonr(test_y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
